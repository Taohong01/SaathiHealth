{
 "metadata": {
  "name": "",
  "signature": "sha256:70c20c3b0b1260a20a8dc7baf5b9404bdf19effadd891437adc97f5a167f0633"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn.cross_validation import train_test_split \n",
      "from nolearn.lasagne import NeuralNet, BatchIterator\n",
      "from lasagne import layers\n",
      "from lasagne.nonlinearities import softmax\n",
      "from lasagne.updates import momentum, nesterov_momentum, sgd, rmsprop\n",
      "import numpy as np\n",
      "from matplotlib import pyplot\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "\n",
      "def plot_loss(net):\n",
      "    \"\"\"\n",
      "    Plot the training loss and validation loss versus epoch iterations with respect to \n",
      "    a trained neural network.\n",
      "    \"\"\"\n",
      "    train_loss = np.array([i[\"train_loss\"] for i in net.train_history_])\n",
      "    valid_loss = np.array([i[\"valid_loss\"] for i in net.train_history_])\n",
      "    pyplot.plot(train_loss, linewidth = 3, label = \"train\")\n",
      "    pyplot.plot(valid_loss, linewidth = 3, label = \"valid\")\n",
      "    pyplot.grid()\n",
      "    pyplot.legend()\n",
      "    pyplot.xlabel(\"epoch\")\n",
      "    pyplot.ylabel(\"loss\")\n",
      "    #pyplot.ylim(1e-3, 1e-2)\n",
      "    pyplot.yscale(\"log\")\n",
      "    pyplot.show()\n",
      "\n",
      "\n",
      "train_df = pd.read_csv('./data/train.csv') \n",
      "test_df = pd.read_csv('./data/test.csv') \n",
      "\n",
      "train_label = train_df.values[:, 0]\n",
      "train_data = train_df.values[:, 1:]\n",
      "print \"train:\", train_data.shape, train_label.shape\n",
      "\n",
      "test_data = test_df.values\n",
      "print \"test:\", test_data.shape\n",
      "\n",
      "train_data = train_data.astype(np.float)\n",
      "train_label = train_label.astype(np.int32)\n",
      "train_data, train_label = shuffle(train_data, train_label, random_state = 21)\n",
      "\n",
      "\n",
      "cnn= NeuralNet(\n",
      "    layers = [  # three layers: one hidden layer\n",
      "        ('input', layers.InputLayer),\n",
      "        ('hidden1', layers.DenseLayer),\n",
      "        ('dropout1', layers.DropoutLayer),\n",
      "        ('hidden2', layers.DenseLayer),\n",
      "        ('dropout2', layers.DropoutLayer),       \n",
      "        ('output', layers.DenseLayer),\n",
      "        ],\n",
      "    # layer parameters:\n",
      "    input_shape = (None, 784),  # 28x28 input pixels per batch\n",
      "    hidden1_num_units = 100,  # number of units in hidden layer\n",
      "    hidden2_num_units = 100,  # number of units in hidden layer\n",
      "    dropout1_p = 0.1, # dropout probability\n",
      "    dropout2_p = 0.1, # dropout probability\n",
      "\n",
      "    output_nonlinearity = softmax,  # output layer uses softmax function\n",
      "    output_num_units = 10,  # 10 labels\n",
      "\n",
      "    # optimization method:\n",
      "    #update = nesterov_momentum,\n",
      "    update = rmsprop,\n",
      "    update_learning_rate = 0.001,\n",
      "    #update_momentum = 0.9,\n",
      "\n",
      "    eval_size = 0.02,\n",
      "\n",
      "    # batch_iterator_train = BatchIterator(batch_size = 20),\n",
      "    # batch_iterator_test = BatchIterator(batch_size = 20),\n",
      "\n",
      "    max_epochs = 200,  # we want to train this many epochs\n",
      "    verbose = 1,\n",
      "    )\n",
      "\n",
      "cnn.fit(train_data, train_label)\n",
      "plot_loss(cnn)\n",
      "\n",
      "pred = cnn.predict(test_data)\n",
      "\n",
      "output = pd.DataFrame(data = {\"ImageId\": range(1, 28001), \"Label\": pred})\n",
      "output.to_csv(\"./fc_2hidden_predict.csv\", index = False, quoting = 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named nolearn.lasagne",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-9b494d5b1176>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnolearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlasagne\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchIterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonlinearities\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named nolearn.lasagne"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}